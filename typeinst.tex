
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\usepackage{wrapfig}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[hyphens]{url}
\usepackage{textcomp}
\usepackage{listings}
\lstset{
        basicstyle=\ttfamily\scriptsize,
        upquote=true,
        showspaces=false,
        showstringspaces=false,
        showtabs=false,
        tabsize=2,
        frame=none,
        breaklines,
        numbers=none,
        framexleftmargin=2mm,
        xleftmargin=2mm,
}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

%% Define a new 'smallurl' style for the package that will use a smaller font.
\makeatletter
\def\url@smallurlstyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\scriptsize\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{smallurl}
\newcommand{\nofootnote}[1]{~#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{A Tweet Consumer's Look At Twitter Through Linked Data Goggles Via Google Analytics}

% a short form should be given in case it is too long for the running head
\titlerunning{A Tweet Consumer's Look At Twitter}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Thomas Steiner \and Arnaud Brousseau}
% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Google Germany GmbH, ABC-Str. 19, 20354 Hamburg, Germany\\
\url{{tomac, arnaudb}@google.com}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%


\maketitle

\begin{abstract}
Twitter Trends\footnote{\url{http://blog.twitter.com/2008/09/twitter-trends-tip.html}} allows for a global or local view on ``what's happening in my world right now" from a tweet producers' point of view. In this paper, we discuss the possibility to complete the functionality provided by Twitter Trends by having a closer look at the other side: the tweet consumers' -- i.e., readers' -- point of view. While Twitter Trends works by analyzing the frequency of terms and their velocity of appearance in tweets being written, our approach is based on the popularity of extracted named entities (in the sense of Linked Data) in tweets being read. Our experimentation architecture takes advantage of the possibility to use a client-side browser extension to harvest and dissect tweets from users' timelines, i.e., tweets supposed to be read. Named entities are extracted via several third-party Natural Language Processing (NLP) Web services in parallel, and are then reported to Google Analytics, which is used to store, analyze, and compute trends by pivoting Analytics data, e.g., users' geographic location, with the recorded named entities.
\end{abstract}

\section{Introduction}
The remainder of the paper is structured as follows:

\subsection{Twitter Trends}

Twitter Trends was introduced by Twitter during the summer of 2008. This service was implemented to reflect ``what's happening in my world right now" on the micro-blogging platform.
To compute trends, Twitter uses an algorithm which analyzes the words and hashtags in tweets. Although the concrete implementation details are kept secret, it is known\footnote{\url{http://blog.twitter.com/2010/12/to-trend-or-not-to-trend.html}} that the algorithm considers three main characteristics of a word/hashtag to determine if it is part of a ``trend":
\clearpage
\begin{itemize}
\item the quantity, i.e, the absolute number of appearances of a given word/hashtag among all users' tweets.
\item the velocity, i.e, the frequency of appearance of that word/hashtag. The higher the frequency, the more popular the word/hashtag.
\item the newness, e.g, the (at the time of writing brand new) hashtag \#ipad2 will be given priority over an old and generic tag like \#awesome to be featured as ``trend". The freshness of trends is also assured by the fact that the frequency is computed over a recent set of tweets, although we have no further details of the exact computation period -- Twitter keeps this information secret.
\end{itemize}
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.17\linewidth]{twitter-trends.png}
  \caption{Screenshot of Twitter Trends as of Friday, March 4, 2011, 4pm CET.}
  \label{fig:dataflow}
\end{figure}

\subsection{Google Chrome Extensions}
Google Chrome extensions\footnote{Google Chrome Extensions: \url{http://code.google.com/chrome/extensions/index.html}. Text partly adapted from the description to be found there.} are small software programs that can be installed to enrich the browsing experience with the Google Chrome browser. They are written using a combination of standard Web technologies, such as HTML, JavaScript, and CSS. Chrome extensions bundle all their resources into a single file that gets usually (but not necessarily) distributed through the Chrome Web Store. There are several types of extensions, for this paper we focus on extensions based on so-called content scripts. Content scripts are JavaScript programs that run in the context of Web pages, similar to the Firefox Greasemonkey extension\footnote{Firefox Greasemonkey extension: \url{http://www.greasespot.net/}}. By using the standard Document Object Model (DOM), they can read or modify details of the Web pages a user visits. Examples of such modifications are, e.g., changing hyperlinks to remove potential @target="\_blank" attributes, or increasing the font size.

\subsection{Google Analytics}
Google Analytics is Google's free Web analysis solution allowing for detailed statistics about the visitors of a website. The software is implemented by including the Google Analytics Tracking Code (GATC), an invisible snippet of JavaScript code that a webmaster needs to add onto the to-be-tracked pages of a website. This code collects visitor data by requesting a specific 1x1 pixel image on Google's servers, where the page and user data is encoded in the query part of the image's URL. In addition to that, the snippet sets a first party cookie on visitors' computers in order to store anonymous information such as the timestamp of the current visit, whether the visitor is a new or returning visitor, and the referrer of the website that the visitor came from. Part of the shared visitor information is the IP address, which allows for IP-based geolocation of visitors. Depending on the region, the accuracy of IP-based geolocation can be down to urban district level.
 
\section{Twitter Swarm NLP Extension}
With our Twitter Swarm NLP extension\footnote{\url{https://chrome.google.com/webstore/detail/dpbphenfafkflfmdlanimlemacankjol}}, we inject JavaScript code via a content script into the Twitter.com homepage. By installing this extension, users explicitely opt-in to their data as a Twitter.com visitor being tracked by Google Analytics. The extension first checks if the user is logged in, and if so, retrieves the tweets of the logged-in user's timeline (\url{http://twitter.com/#}), or search result page (e.g., \url{http://twitter.com/#!/search/twitter}), or profile page (e.g., \url{http://twitter.com/#!/timberners_lee}) one-by-one, and performs NLP analysis via a remote NLP Web service (see section \ref{sec:webservice}) on each of the tweets. The extracted entities are then displayed on the righthand-pane of the Twitter.com homepage (see the left part of figure \ref{fig:screenshot}), and sent to Google Analytics for further processing (see the right part of figure \ref{fig:screenshot}).

\subsection{Twitter Swarm NLP Web Service}\label{sec:webservice}
We have created a wrapper NLP Web service that merges results from existing third-party NLP Web services, namely from OpenCalais\footnote{\url{http://www.opencalais.com/}}, Zemanta\footnote{\url{http://www.opencalais.com/}}, AlchemyAPI\footnote{\url{http://www.alchemyapi.com/}}, and DBpedia Spotlight\footnote{\url{http://dbpedia.org/spotlight}}. While the original calls to each particular NLP service are all HTTP \texttt{POST}-based, we have implemented the wrapper service \texttt{GET}- and \texttt{POST}-based. All NLP Web services return entities with their types and/or subtypes, names, relevance, and URIs that link into the LOD cloud. The problem is that each service has implemented its own type system, and providing mappings for all of them would be a rather time-consuming task. However, as all services offer links into the LOD cloud\footnote{\url{http://lod-cloud.net/}}, the desired type information can be pulled from there in a true Linked Data manner. The least common multiple of the results for the sample query ``Google Translate'' (i.e., the result of the call to the Web service at \url{http://tomayac.no.de/entity-extraction/combined/Google%20Translate}) is depicted below. For the sake of clarity, we just show one entity with two URIs while the original result contained seven entities among which six were directly relevant and one was closely related.

\begin{lstlisting}
[
  {
    "name": "Google Translate",
    "relevance": 0.7128319999999999,
    "uris": [
      {
        "uri": "http://dbpedia.org/resource/Google_Translate",
        "source": "alchemyapi"
      },
      {
        "uri": "http://rdf.freebase.com/ns/en/google_translate",
        "source": "zemanta"
      }
    ],
    "source": "alchemyapi,zemanta"
  }
]
\end{lstlisting}

\subsection{Technical Implementation Of the Twitter Swarm NLP Extension}
Twitter.com is an Ajax-dependent website,\footnote{See \url{http://www.jenitennison.com/blog/node/154} for a detailed analysis of Twitter's use of hashbang URIs} which makes use of so-called hashbang URIs. Currently the extension is implemented in a way to run once upon page load, however, not upon Ajax refreshes of the page. Each tweet gets sent one-by-one to the Twitter Swarm NLP Web service, which is described in section \ref{sec:webservice}.

\subsection{Dealing With Extracted Entities On the Client Side}

\subsection{Dealing With Extracted Entities On the Google Analytics Side}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\linewidth]{combined.png}
  \caption{Left: Screenshot of the extracted entites of a particular tweet as displayed by the Twitter Swarm NLP Extension. Right: Entities pivoted by country. The entity represented by the URL \texttt{http://dbpedia.org/resource/Lybia} appeared in nine tweets on timelines of users located in Finland (red borders in the screenshot).}
  \label{fig:screenshot}
\end{figure}

\section{Related Work}

\subsection{Linked Open Social Signals (TWARQL)}
In the Linked Open Social Signals project~\cite{Mendes:LOSS} Mendes et al. investigate the representation of microposts as Linked Open Data (the authors call the opinions, observations, and suggestions contained in microposts ``social signals", hence the project name Linked Open Social Signals). Mendes et al. address the problem of information overload caused by the sheer amount of microposts. While the micropost community has come up with hashtags in order to categorize microposts, these hashtags are ambigious and have to be explicitely added to the micropost by the author. Given the typical length limitations of microposts (often 140 characters), sometimes hashtags are left out in favor of more text. The project's main goal is thus to enable collective analysis of social signals for sensemaking by making use of Linked Open Data principles in combination with realtime push models. The approach consists of the following steps:\footnote{Steps adapted from \url{http://wiki.knoesis.org/index.php/Twarql}} 
\begin{itemize}
\item Extract content (entity mentions, hashtags and URLs) from microposts
\item Encode content in RDF format using common vocabularies
\item Enable SPARQL querying of microposts
\item Enable subscription to micropost streams that match a given query
\item Enable scalable real-time delivery of streaming data via SparqlPuSH
\end{itemize}
The authors maintain a client-side JavaScript application allowing for users to search for tweets matching a customizable SPARQL query or to subscribe to tweet streams in a realtime ``push" way filtered according to the user's request (so-called concept feeds).

\subsection{Twitris 2.0}
With Twitris 2.0~\cite{Jadhav:Twitris}, Jadhav et al. present an application to find out what is when being said about an event, to detect how topics of discussion are changing over a period of time, and finally to check whether there are regional differences in the opinions on a given topic. The approach consists of picking trending hashtags from Twitter, which are then expanded by data from Google Insights for Search\footnote{\url{http://www.google.com/insights/search/}}. Using this set of search terms, find related hashtags by performing a Twitter search in order to detect topic drifts. In order to locate the origin of a tweet, Twitris uses the approximation of the location given in the user's Twitter profile. This approach works well if there is geocodable data (like ``Austin, Texas"), however, fails if there is generic data (like ``somewhere under the rainbow"). Tweets about a given set of topics can then be examined on a map view, enriched by relevant photo and video content. For each tweet location hotspot a tag cloud with related tags is displayed, and the data can be sliced by time.

\subsection{Semantic-MicrOBlogging (SMOB)}
SMOB~\cite{Passant2008} by Passant et al. is a Semantic MicroBlogging framework that enables a distributed, open and semantic microblogging experience based on Semantic Web and Linked Data technologies. Microposts get annotated with common vocabularies like FOAF\footnote{\url{http://www.foaf-project.org/}} and SIOC\footnote{\url{http://sioc-project.org/}}. SMOB relies on distributed hubs that communicate with each other to exchange microblog posts and subscriptions, which can also be cross-posted to Twitter. The authors suggest people to use meaningful hashtags such as \texttt{\#dbp:Eiffel\_Tower}, or \texttt{\#geo:Paris\_France}, in the style of RDF prefixes for DBpedia~\cite{Bizer:DBpedia} and GeoNames. SMOB allows for manually annotating hashtags with URIs and RDF, with the objective of making microposts accessible for, e.g., lookup services such as Sindice~\cite{Tummarello:Sindice}.

\subsection{Twopular}
Twopular\footnote{\url{http://twopular.com/}} is an experiment by Martin Dudek with the objective of analyzing current Twitter trends. Therefore, for a given set of at the particular moment current trends the most recents tweets are obtained, and in an interval of five minutes run through the OpenCalais Web service in order find tags. By having tags for Twitter trends, another way of searching for trends -- and more importantly the possibility to interrelate trends based on tag similarity -- gets enabled. The author sees the feature more like a ``linguistic experiment"\footnote{\url{http://twopular.com/blog/?p=308}}, however states that the first results seem to be promising. In our tests of the service we could affirm the author's self-assessment, e.g., the Twitter trend (at the time of writing, March 7, 2011, 2PM CET) ``Prince Andrew" was mapped to the OpenCalais tag ``Prince Andrew, Duke of York", and related tags were, among others,  ``Sex offender registration", and ``X-Offender", where the story behind the trend was that people were tweeting about Prince Andrew of England, whose close friend was found to be a pedophile.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{twopular-trends.png}
  \caption{Screenshot of the Twopular trends page.}
  \label{fig:dataflow}
\end{figure}

\section{Conclusion}
Contributions: time filters (via Analytics), geographical pivoting (via Analytics)

As seen in the Related Work section, semantic analysis of a (real-time) Twitter stream is not new and has been successfully exploited to analyse tweets produced by the Twitter community.
What we propose here is an insight into tweets consumers' interests to provide a more accurate view of Twitter trends.

%%%%%%%%%%%%%%%%%%%%%%
%%%  Bibliography  %%%
%%%%%%%%%%%%%%%%%%%%%%

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{typeinst}

\end{document}
